{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1a9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "import urllib\n",
    "from urllib3.exceptions import ConnectionError\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.python.scraper.utils import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from pycookiecheat import chrome_cookies\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb074f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = os.getcwd() + \"/data/text/solomon_islands/\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.mkdir(target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e254c",
   "metadata": {},
   "source": [
    "## Solomon Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    params = {\n",
    "        \"title_entry\": \"blog-content wf-td\",\n",
    "        \"title\": \"entry-title\",\n",
    "        \"date\": \"entry-date\"\n",
    "    }\n",
    "\n",
    "    host = \"https://www.solomonstarnews.com/category/news/news-national/page/\"\n",
    "    urls = [host + str(i) for i in range(1, 1421)]\n",
    "    max_workers = multiprocessing.cpu_count() - 1\n",
    "    output = []\n",
    "\n",
    "\n",
    "    with tqdm(total=len(urls), unit=\"pages\") as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_url = {\n",
    "                executor.submit(extract_news_info, url, params=params, timeout=5):\n",
    "                url\n",
    "                for url in urls\n",
    "            }\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as exc:\n",
    "                    print('%r generated an exception: %s' % (url, exc))\n",
    "                else:\n",
    "                    output.append(data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()\n",
    "for out in output:\n",
    "    out_df = pd.DataFrame(out)\n",
    "    if output_df.empty:\n",
    "        output_df = out_df\n",
    "    else:\n",
    "        output_df = pd.concat([output_df, out_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b29b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.reset_index().drop(\"index\", axis=1)\n",
    "output_df[\"date\"] = pd.to_datetime(output_df[\"date\"])\n",
    "output_df.sample(10)\n",
    "output_df.to_csv(target_dir+\"solomon_stars_urls.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3307b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news(url):\n",
    "    content = load_page(url, timeout=5)\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    try: \n",
    "        raw_text = soup.find(class_=\"entry-content\").text\n",
    "    except:\n",
    "        raw_text = soup.find(class_=\"content\").text\n",
    "    return url, raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d344f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_lst = []\n",
    "with tqdm(total=len(output_df)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()+4) as executor:  \n",
    "        futures = {executor.submit(extract_news, url): url for url in output_df.url}\n",
    "        for future in as_completed(futures):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                url, news = future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "            else:\n",
    "                news_lst.append((url, news))\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eebcfa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(news_lst, columns=[\"url\", \"news\"])\n",
    "display(news_df.head(5))\n",
    "output_df = output_df.merge(news_df, how=\"left\", on=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ae736",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "missed_idx = output_df[output_df.news.isna() == True].index.tolist()\n",
    "for idx in missed_idx:\n",
    "    url = output_df[\"url\"][idx]\n",
    "    _ , news = extract_news(url)\n",
    "    if math.isnan(output_df.iloc[idx, -1]):\n",
    "        output_df.iloc[idx, -1] = news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aee4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_df[output_df.news.isna() == True]) == 0:\n",
    "    output_df.to_csv(target_dir+\"solomon_stars_news.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de308a4f",
   "metadata": {},
   "source": [
    "## Solomon Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = \"https://www.solomontimes.com/news/latest/\"\n",
    "\n",
    "ym_lst = [(i, j) for i in range(2007, 2023) for j in range(1, 13)]\n",
    "ym_lst.extend([(2023, m) for m in range(1, 9)])\n",
    "ss_urls = [host_url + str(y) + \"/\" + str(m) for y, m in ym_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(url, timeout):\n",
    "    r = requests.get(url)\n",
    "    return r.content\n",
    "\n",
    "def extract_news_info_ss(url, params=None, timeout=5):\n",
    "    content = load_page(url, timeout)\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    info_dict = {\"url\": [], \"title\": [], \"date\": []}\n",
    "    if params is None:\n",
    "        params = {\"title_entry\": \"entry-container\",\n",
    "                  \"title\": \"entry-title\",\n",
    "                  \"date\": \"entry-date\"}\n",
    "    for a in soup.find_all(class_=params[\"title_entry\"]):\n",
    "        entry_title = a.find(params[\"title\"])\n",
    "        if params[\"date\"] is not None:\n",
    "            info_dict[\"date\"].append(a.find(class_=params[\"date\"]).text)\n",
    "        else:\n",
    "            info_dict[\"date\"].append(np.NaN)\n",
    "        \n",
    "        info_dict[\"url\"].append(a.find(\"a\").attrs[\"href\"])\n",
    "        \n",
    "        if entry_title is not None:\n",
    "            info_dict[\"title\"].append(entry_title.text)\n",
    "        else:\n",
    "            info_dict[\"title\"].append(\"Missing Title.\")\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"title_entry\": \"article-list-item\",\n",
    "    \"title\": 'h2',\n",
    "    \"date\": None\n",
    "}\n",
    "\n",
    "urls_df = pd.DataFrame()\n",
    "with tqdm(total=len(ss_urls), unit=\"pages\") as pbar:\n",
    "    for url, ym in zip(ss_urls, ym_lst):\n",
    "        url_dict = extract_news_info_ss(url, params=params)\n",
    "        url_df = pd.DataFrame(url_dict)\n",
    "        url_df[\"date\"] = str(ym)\n",
    "        if urls_df.empty:\n",
    "            urls_df = url_df\n",
    "        else:\n",
    "            urls_df = pd.concat([urls_df, url_df], axis=0)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df_clean = urls_df.reset_index().drop(\"index\", axis=1)\n",
    "urls_df_clean[\"date\"] = [\n",
    "    d.replace(\"(\", \"\").replace(\")\", \"\").replace(\", \", \"-\").strip()\n",
    "    for d in urls_df_clean.date\n",
    "]\n",
    "urls_df_clean[\"url\"] = [\n",
    "    \"https://www.solomontimes.com\" + url for url in urls_df_clean.url\n",
    "]\n",
    "urls_df_clean.to_csv(target_dir+\"solomon_times_urls.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_ss(url):\n",
    "    content = load_page(url, 5)\n",
    "    soup = BeautifulSoup(content)\n",
    "\n",
    "    date = soup.find(class_=\"article-timestamp\").find(\"span\")[\"datetime\"]\n",
    "    text = soup.find(class_=\"article-body\").text.strip()\n",
    "    tags = soup.find(class_=\"tags\").find_all(\"a\")\n",
    "    tags_text = \"\".join(t.text + \",\" if i < len(tags) - 1 else t.text\n",
    "                        for i, t in enumerate(tags))\n",
    "\n",
    "    return {\"url\": url, \"date\": date, \"news\": text, \"tag\": tags_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_st = []\n",
    "with tqdm(total=len(urls_df_clean)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()+4) as executor:  \n",
    "        futures = {executor.submit(extract_news_ss, url): url for url in urls_df_clean.url}\n",
    "        for future in as_completed(futures):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                news = future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "            else:\n",
    "                news_st_dict.append(news)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbaab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_news = pd.DataFrame(news_ss_dict)\n",
    "st_news[\"date\"] = pd.to_datetime(st_news[\"date\"])\n",
    "st_news.to_csv(target_dir + \"solomon_times_news.csv\", encoding=\"utf-8\")\n",
    "st_news.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f1d49",
   "metadata": {},
   "source": [
    "## The Island Sun\n",
    "<b> !!! Need to Bypass Cloudflare by using cookies obtained from [Google Chrome](chrome://settings/content/siteDetails?site=https%3A%2F%2Ftheislandsun.com.sb%2F) and Headers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97be3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = 'https://theislandsun.com.sb/'\n",
    "stored_cookies = \"/Users/czhang/Library/Application Support/Google/Chrome/Profile 7/Cookies\"\n",
    "\n",
    "\n",
    "def configure_cf(host_url, cookies_path):\n",
    "    cookies = chrome_cookies(host_url, cookies_path)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n",
    "        \"sec-ch-ua-platform\":\n",
    "        \"MacOS\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        \"dnt\": \"1\",\n",
    "        \"sec-ch-ua\":\n",
    "        '\"Not.A/Brand\";v=\"24\", \"Chromium\";v=\"116\", \"Google Chrome\";v=\"116\"',\n",
    "        \"Accept\":\n",
    "        \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"cf-ray\": \"7fc606513b832000-iad\"\n",
    "    }\n",
    "\n",
    "    return headers, cookies\n",
    "\n",
    "\n",
    "def load_page_cf(url, headers, cookies):\n",
    "    r = requests.get(url, cookies=cookies, headers=headers)\n",
    "    if r.status_code == 200:\n",
    "        return r.content\n",
    "    else:\n",
    "        raise ConnectionError\n",
    "\n",
    "\n",
    "def extract_url_cf(content):\n",
    "    news_info_dict = {\"title\": [], \"url\": [], \"date\": [], \"tag\": []}\n",
    "    soup = BeautifulSoup(content)\n",
    "    for ele in soup.find_all(class_=\"item-details\"):\n",
    "        title_entry = ele.find(class_=\"entry-title td-module-title\")\n",
    "        tag_entry = ele.find(class_=\"td-post-category\")\n",
    "        url_entry = title_entry.find(\"a\")\n",
    "        date_entry = ele.find(class_=\"td-post-date\")\n",
    "        if ele is not None:\n",
    "            news_info_dict[\"title\"].append(title_entry.text)\n",
    "            news_info_dict[\"url\"].append(url_entry[\"href\"])\n",
    "            news_info_dict[\"date\"].append(date_entry.text)\n",
    "            news_info_dict[\"tag\"].append(tag_entry.text)\n",
    "    return news_info_dict\n",
    "\n",
    "\n",
    "def scrape_cf(url):\n",
    "    host_url = 'https://theislandsun.com.sb/'\n",
    "    stored_cookies = \"/Users/czhang/Library/Application Support/Google/Chrome/Profile 7/Cookies\"\n",
    "    headers, cookies = configure_cf(host_url, stored_cookies)\n",
    "    content = load_page_cf(url, headers=headers, cookies=cookies)\n",
    "    news_info_dict = extract_url_cf(content)\n",
    "    return news_info_dict\n",
    "\n",
    "\n",
    "def extract_text(url, cookies=None):\n",
    "    if cookies is None:\n",
    "        headers, cookies = configure_cf(host_url, stored_cookies)\n",
    "    else:\n",
    "        headers, _ = configure_cf(host_url, stored_cookies)\n",
    "    content = load_page_cf(url, headers=headers, cookies=cookies)\n",
    "    soup = BeautifulSoup(content)\n",
    "    p_container = soup.find(class_=\"td-post-content tagdiv-type\")\n",
    "    text = \"\".join(p.text for p in p_container.find_all(\"p\"))\n",
    "    return {\"url\": url, \"news\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69449032",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls = [\n",
    "    host_url + \"category/news/page/\" + str(num) for num in range(1, 903)\n",
    "]\n",
    "output = []\n",
    "with tqdm(total=len(page_urls), unit=\"pages\") as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()+4) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(scrape_cf, url):\n",
    "            url\n",
    "            for url in page_urls\n",
    "        }\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "            else:\n",
    "                output.append(data)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_url_df = pd.DataFrame()\n",
    "for i in output:\n",
    "    temp_df = pd.DataFrame(i)\n",
    "    if news_url_df.empty:\n",
    "        news_url_df = temp_df\n",
    "    else:\n",
    "        news_url_df = pd.concat([news_url_df, temp_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6988fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_url_df[\"date\"] = pd.to_datetime(news_url_df[\"date\"])\n",
    "news_url_df = news_url_df.sort_values(by=\"date\", ascending=True).reset_index(drop=True)\n",
    "news_url_df.to_csv(target_dir + \"island_sun_urls.csv\", encoding=\"utf-8\")\n",
    "news_url_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cryptography.hazmat.primitives.ciphers import Cipher, modes\n",
    "# from cryptography.hazmat.primitives.ciphers.algorithms import AES\n",
    "# from cryptography.hazmat.primitives.hashes import SHA1\n",
    "# from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "# import sqlite3\n",
    "\n",
    "# conn = sqlite3.connect(stored_cookies)\n",
    "# cookies = conn.execute(\"SELECT * FROM cookies;\").fetchall()\n",
    "\n",
    "# coi = []\n",
    "# for item in cookies:\n",
    "#     if \"island\" in item[1]:\n",
    "#         coi.append(item)\n",
    "        \n",
    "# conn.close()\n",
    "\n",
    "\n",
    "# kdf = PBKDF2HMAC(algorithm=SHA1(), salt=b'saltysalt', iterations=1003, length=16)\n",
    "# key = kdf.derive()\n",
    "# def chrome_decrypt(encrypted_value: bytes,\n",
    "#                    key: bytes,\n",
    "#                    init_vector=b\" \" * 16) -> str:\n",
    "\n",
    "#     encrypted_value = encrypted_value[3:]\n",
    "\n",
    "#     cipher = Cipher(\n",
    "#         algorithm=AES(key),\n",
    "#         mode=modes.CBC(init_vector),\n",
    "#     )\n",
    "#     decryptor = cipher.decryptor()\n",
    "#     decrypted = decryptor.update(encrypted_value) + decryptor.finalize()\n",
    "\n",
    "#     return clean(decrypted)\n",
    "# cipher = Cipher(\n",
    "#     algorithm=AES(key),\n",
    "#     mode=modes.CBC( b\" \" * 16),\n",
    "# )\n",
    "# decryptor = cipher.decryptor() \n",
    "# decrypted = decryptor.update(coi[0][5][3:]) + decryptor.finalize()\n",
    "# def clean(decrypted: bytes) -> str:\n",
    "#     last = decrypted[-1]\n",
    "#     if isinstance(last, int):\n",
    "#         return decrypted[:-last].decode(\"utf-8\")\n",
    "#     return decrypted[: -ord(last)].decode(\"utf-8\")\n",
    "\n",
    "# clean(decrypted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90d2c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(target_dir + \"island_sun_news.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf6c3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_url = 'https://theislandsun.com.sb/'\n",
    "stored_cookies = \"/Users/czhang/Library/Application Support/Google/Chrome/Profile 7/Cookies\"\n",
    "\n",
    "headers, cookies = configure_cf(host_url, stored_cookies)\n",
    "cookies[\"cf_clearance\"] = \"qAG73VfB9SLEcqIspcGrR.LT.3ymhfmv3E7Na4neGCM-1692998026-0-1-c5447e81.817438c3.a04331c9-160.2.1692998026\"\n",
    "cookies[\"cf_chl_2\"] = \"46c9bf8020b3cbb\"\n",
    "cookies[\"_gid\"] = \"GA1.3.1687955229.1692987941\"\n",
    "cookies[\"_ga\"] = \"GA1.1.1982791408.1692825085\"\n",
    "cookies[\"_ga_7LT5NMM1C2\"] = \"GS1.1.1692994235.9.1.1692994241.0.0.0\"\n",
    "cookies[\"_gat_gtag_UA_162935749_1\"] = \"1\"\n",
    "cookies[\"__cf_bm\"] = \"3GFEz5Oc63ext35xM1Bl.BCHsDYNTUFVXd5OJGnEl1g-1692995875-0-AcIDZkypNt/HsdXScCpuCiVpPIfD9gqjYQdMrCTzpGIq4ivtRrGms1lQ8aizPLh9W5kBc4vG1XIZR4RVkn/Cqdk=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cd20477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.81s/pages]\n"
     ]
    }
   ],
   "source": [
    "news_urls = news_df[news_df[\"news\"].isna() == True].url.tolist()\n",
    "with tqdm(total=len(news_urls), unit=\"pages\") as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()+4) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(extract_text, url, cookies=cookies):\n",
    "            url\n",
    "            for url in news_urls\n",
    "        }\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "            else:\n",
    "                news_df.loc[news_df.url == url, \"news\"] = data[\"news\"]\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70d74638",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv(target_dir + \"island_sun_news.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "po",
   "language": "python",
   "name": "po"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
